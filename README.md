# Data Warehouse Automation With Airflow DAG Project

Welcome to the **Data Warehouse Automation With Airflow DAG Project (Data Engineer)** repository! ğŸš€  
This project showcases a complete data engineering workflow from ingesting raw data into a Data Lake, transforming it via PySpark, orchestrating pipelines with Apache Airflow, to storing structured data in a Data Warehouse for analytics.  

---
## ğŸ—ï¸ Data Architecture

The data architecture for this project follows Medallion Architecture **Bronze**, **Silver**, and **Gold** layers:
![Image](https://github.com/user-attachments/assets/8621fdbb-42d7-47b7-934d-d76b248c66fb)

1. **Bronze Layer**: Stores raw data as-is from the source systems. Data is ingested from CSV Files into Minio bucket raw.
2. **Silver Layer**: This layer includes data cleansing, standardization, and normalization processes to prepare data for analysis.
3. **Gold Layer**: Houses business-ready data modeled into a star schema required for reporting and analytics. Adding
4. **Airflow**: For orchestrates the ELT pipeline, Supports scheduled and DAG execution

---

## ğŸ“‚ Repository Structure
```
data_engineering/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€airflow_venv/               â† Airflow virtual environment
â”‚   â”‚    â””â”€â”€ airflowhome/          â† Airflow logs, airflows.cfg, airflow.db, webserver_config.py
â”‚   â””â”€â”€ dags/                      â† DAG folder for Airflow workflows
â”‚   â””â”€â”€ run-airflow.sh             # Script to run Airflow
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ sources_crm/               â† raw data crm
â”‚   â”‚    â””â”€â”€ cust_info.csv/   
â”‚   â”‚    â””â”€â”€ prd_info.csv/   
â”‚   â”‚    â””â”€â”€ sales_details.csv/   
â”‚   â”œâ”€â”€ sources_erp/               â† raw data erp
â”‚   â”‚    â””â”€â”€ CUST_AZ12.csv/   
â”‚   â”‚    â””â”€â”€ LOC_A101.csv/   
â”‚   â”‚    â””â”€â”€ PX_CAT_G1V2.csv/  
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_venv/                  â† dbt virtual environment
â”‚   â”œâ”€â”€ logs/                      â† dbt.log
â”‚   â””â”€â”€ my_dbt_project/            â† The dbt project contains models, seeds, etc
â”œâ”€â”€ duckdb/
â”‚   â””â”€â”€ db/                        â† datawarehouse
â”œâ”€â”€ minio/
â”‚   â”œâ”€â”€ data/                      # Data folder for MinIO
â”‚   â”œâ”€â”€ minio                      # Binary MinIO
â”‚   â””â”€â”€ run-minio.sh               # Script to run Airflow
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ spark_venv/                # PySpark, JupyterLab, duckdb, ipykernel, numpy, pandas, matplotlib
â”‚   â”œâ”€â”€ notebooks/                 # .ipynb for ELT, analysis with PySpark/DuckDB
â”‚   â”œâ”€â”€ scripts/                   # File Python (.py)
â”‚   â””â”€â”€ run-jupyter.sh             # Script to run JupyterLab
```
---
## ğŸ“Š Output & Insights
This pipeline simulates a real-world batch data pipeline and prepares structured data for further analysis and visualization in tools like Power BI or Metabase.
