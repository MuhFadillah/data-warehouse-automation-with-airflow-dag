{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7392b47d-1247-4a16-857d-126e96b8a873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 13:00:33 WARN Utils: Your hostname, lenovo-PC resolves to a loopback address: 127.0.1.1; using 172.31.112.46 instead (on interface eth0)\n",
      "25/05/26 13:00:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntucoy/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntucoy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntucoy/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-37e7bd52-109c-4173-88ec-86848e2c1f1b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 1283ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.11.1026] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-37e7bd52-109c-4173-88ec-86848e2c1f1b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/36ms)\n",
      "25/05/26 13:00:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 13:00:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/26 13:01:07 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/26 13:01:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ELT_PySpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c58628c-b941-4f0b-a11e-fbe16344e654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 13:01:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|          CID|     BDATE|   GEN|\n",
      "+-------------+----------+------+\n",
      "|NASAW00011000|1971-10-06|  Male|\n",
      "|NASAW00011001|1976-05-10|  Male|\n",
      "|NASAW00011002|1971-02-09|  Male|\n",
      "|NASAW00011003|1973-08-14|Female|\n",
      "|NASAW00011004|1979-08-05|Female|\n",
      "|NASAW00011005|1976-08-01|  Male|\n",
      "|NASAW00011006|1976-12-02|Female|\n",
      "|NASAW00011007|1969-11-06|  Male|\n",
      "|NASAW00011008|1975-07-04|Female|\n",
      "|NASAW00011009|1969-09-29|  Male|\n",
      "|NASAW00011010|1969-08-05|Female|\n",
      "|NASAW00011011|1969-05-03|  Male|\n",
      "|NASAW00011012|1979-01-14|Female|\n",
      "|NASAW00011013|1979-08-03|  Male|\n",
      "|NASAW00011014|1973-11-06|Female|\n",
      "|NASAW00011015|1984-08-26|Female|\n",
      "|NASAW00011016|1984-10-25|  Male|\n",
      "|NASAW00011017|1949-12-24|Female|\n",
      "|NASAW00011018|1955-10-06|  Male|\n",
      "|NASAW00011019|1983-09-04|  Male|\n",
      "+-------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"s3a://raw/erp/CUST_AZ12.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b07f6f9-2052-4b45-97d9-d26ccdd4ab08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|       cid|     bdate|   gen|\n",
      "+----------+----------+------+\n",
      "|AW00011000|1971-10-06|  Male|\n",
      "|AW00011001|1976-05-10|  Male|\n",
      "|AW00011002|1971-02-09|  Male|\n",
      "|AW00011003|1973-08-14|Female|\n",
      "|AW00011004|1979-08-05|Female|\n",
      "|AW00011005|1976-08-01|  Male|\n",
      "|AW00011006|1976-12-02|Female|\n",
      "|AW00011007|1969-11-06|  Male|\n",
      "|AW00011008|1975-07-04|Female|\n",
      "|AW00011009|1969-09-29|  Male|\n",
      "|AW00011010|1969-08-05|Female|\n",
      "|AW00011011|1969-05-03|  Male|\n",
      "|AW00011012|1979-01-14|Female|\n",
      "|AW00011013|1979-08-03|  Male|\n",
      "|AW00011014|1973-11-06|Female|\n",
      "|AW00011015|1984-08-26|Female|\n",
      "|AW00011016|1984-10-25|  Male|\n",
      "|AW00011017|1949-12-24|Female|\n",
      "|AW00011018|1955-10-06|  Male|\n",
      "|AW00011019|1983-09-04|  Male|\n",
      "+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, substring, length, upper, trim, expr, current_date\n",
    "\n",
    "# Transformasi\n",
    "df_transformed = df.select(\n",
    "    when(\n",
    "        col(\"cid\").like(\"NAS%\"),\n",
    "        expr(\"SUBSTRING(cid, 4, length(cid))\")\n",
    "    ).otherwise(\n",
    "        col(\"cid\")\n",
    "    ).alias(\"cid\"),       # cid: Remove 'NAS' prefix jika ada\n",
    "    \n",
    "    when(\n",
    "        col(\"bdate\") > current_date(),\n",
    "        None\n",
    "    ).otherwise(\n",
    "        col(\"bdate\")\n",
    "    ).alias(\"bdate\"),     # bdate: Set future birthdates to NULL\n",
    "    \n",
    "    when(\n",
    "        upper(trim(col(\"gen\"))).isin(\"F\", \"FEMALE\"),\n",
    "        \"Female\"\n",
    "    ).when(\n",
    "        upper(trim(col(\"gen\"))).isin(\"M\", \"MALE\"),\n",
    "        \"Male\"\n",
    "    ).otherwise(\n",
    "        \"n/a\"\n",
    "    ).alias(\"gen\")        # gen: Normalize gender values\n",
    ")\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff51680f-0db9-4dca-9faa-fe2191ee16ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 13:01:44 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/05/26 13:01:44 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_transformed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"s3a://clean/erp/CUST_AZ12_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5695727-9521-4316-84e5-1959f251d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|       cid|     bdate|   gen|\n",
      "+----------+----------+------+\n",
      "|AW00011000|1971-10-06|  Male|\n",
      "|AW00011001|1976-05-10|  Male|\n",
      "|AW00011002|1971-02-09|  Male|\n",
      "|AW00011003|1973-08-14|Female|\n",
      "|AW00011004|1979-08-05|Female|\n",
      "|AW00011005|1976-08-01|  Male|\n",
      "|AW00011006|1976-12-02|Female|\n",
      "|AW00011007|1969-11-06|  Male|\n",
      "|AW00011008|1975-07-04|Female|\n",
      "|AW00011009|1969-09-29|  Male|\n",
      "|AW00011010|1969-08-05|Female|\n",
      "|AW00011011|1969-05-03|  Male|\n",
      "|AW00011012|1979-01-14|Female|\n",
      "|AW00011013|1979-08-03|  Male|\n",
      "|AW00011014|1973-11-06|Female|\n",
      "|AW00011015|1984-08-26|Female|\n",
      "|AW00011016|1984-10-25|  Male|\n",
      "|AW00011017|1949-12-24|Female|\n",
      "|AW00011018|1955-10-06|  Male|\n",
      "|AW00011019|1983-09-04|  Male|\n",
      "+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = spark.read.option(\"header\", True).csv(\"s3a://clean/erp/CUST_AZ12_clean.csv\")\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53a30a1-100c-4561-b264-6ae128469cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7f2035a55ab0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Convert dari Spark ke Pandas\n",
    "df_cleaned_CUST_AZ12 = df_cleaned.toPandas()\n",
    "\n",
    "# Connect ke DuckDB\n",
    "con = duckdb.connect(\"/mnt/d/data_engineering/duckdb/db/dev.duckdb\")\n",
    "\n",
    "# Simpan sebagai tabel DuckDB (view)\n",
    "con.register(\"erp_CUST_AZ12_clean_view\", df_cleaned_CUST_AZ12)\n",
    "\n",
    "# Simpan sebagai tabel DuckDB (bukan view)\n",
    "con.execute(\"CREATE OR REPLACE TABLE erp_CUST_AZ12_clean AS SELECT * FROM erp_CUST_AZ12_clean_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41028b15-81f5-45e8-8fdf-728d38624dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fc2a7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_fc2a7_level0_col0\" class=\"col_heading level0 col0\" >cid</th>\n",
       "      <th id=\"T_fc2a7_level0_col1\" class=\"col_heading level0 col1\" >bdate</th>\n",
       "      <th id=\"T_fc2a7_level0_col2\" class=\"col_heading level0 col2\" >gen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row0_col0\" class=\"data row0 col0\" >AW00011000</td>\n",
       "      <td id=\"T_fc2a7_row0_col1\" class=\"data row0 col1\" >1971-10-06</td>\n",
       "      <td id=\"T_fc2a7_row0_col2\" class=\"data row0 col2\" >Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row1_col0\" class=\"data row1 col0\" >AW00011001</td>\n",
       "      <td id=\"T_fc2a7_row1_col1\" class=\"data row1 col1\" >1976-05-10</td>\n",
       "      <td id=\"T_fc2a7_row1_col2\" class=\"data row1 col2\" >Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row2_col0\" class=\"data row2 col0\" >AW00011002</td>\n",
       "      <td id=\"T_fc2a7_row2_col1\" class=\"data row2 col1\" >1971-02-09</td>\n",
       "      <td id=\"T_fc2a7_row2_col2\" class=\"data row2 col2\" >Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row3_col0\" class=\"data row3 col0\" >AW00011003</td>\n",
       "      <td id=\"T_fc2a7_row3_col1\" class=\"data row3 col1\" >1973-08-14</td>\n",
       "      <td id=\"T_fc2a7_row3_col2\" class=\"data row3 col2\" >Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row4_col0\" class=\"data row4 col0\" >AW00011004</td>\n",
       "      <td id=\"T_fc2a7_row4_col1\" class=\"data row4 col1\" >1979-08-05</td>\n",
       "      <td id=\"T_fc2a7_row4_col2\" class=\"data row4 col2\" >Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row5_col0\" class=\"data row5 col0\" >AW00011005</td>\n",
       "      <td id=\"T_fc2a7_row5_col1\" class=\"data row5 col1\" >1976-08-01</td>\n",
       "      <td id=\"T_fc2a7_row5_col2\" class=\"data row5 col2\" >Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row6_col0\" class=\"data row6 col0\" >AW00011006</td>\n",
       "      <td id=\"T_fc2a7_row6_col1\" class=\"data row6 col1\" >1976-12-02</td>\n",
       "      <td id=\"T_fc2a7_row6_col2\" class=\"data row6 col2\" >Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row7_col0\" class=\"data row7 col0\" >AW00011007</td>\n",
       "      <td id=\"T_fc2a7_row7_col1\" class=\"data row7 col1\" >1969-11-06</td>\n",
       "      <td id=\"T_fc2a7_row7_col2\" class=\"data row7 col2\" >Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row8_col0\" class=\"data row8 col0\" >AW00011008</td>\n",
       "      <td id=\"T_fc2a7_row8_col1\" class=\"data row8 col1\" >1975-07-04</td>\n",
       "      <td id=\"T_fc2a7_row8_col2\" class=\"data row8 col2\" >Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fc2a7_row9_col0\" class=\"data row9 col0\" >AW00011009</td>\n",
       "      <td id=\"T_fc2a7_row9_col1\" class=\"data row9 col1\" >1969-09-29</td>\n",
       "      <td id=\"T_fc2a7_row9_col2\" class=\"data row9 col2\" >Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f20359f4f10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM erp_CUST_AZ12_clean LIMIT 10\").df().style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ee094a-dc64-4237-97fb-c16fd9933d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c54b5-d6ef-4634-bfb2-03fb5d9bfde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (spark_venv)",
   "language": "python",
   "name": "spark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
